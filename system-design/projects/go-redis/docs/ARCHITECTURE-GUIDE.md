# Redis 系统架构设计学习指南

## 🎯 学习目标

**本项目不是 Redis 克隆，而是通过 Redis 案例学习系统架构设计**

你将学会：
- 如何分析一个复杂系统的架构需求
- 如何权衡不同架构选择的 Trade-offs
- 如何设计可扩展、高可用、高性能的系统
- 如何将架构思维应用到其他分布式系统

---

## 📐 架构设计思维框架

### 第一步：需求分析

**不要直接写代码！先问这些问题：**

#### 1. 功能性需求（Functional Requirements）
- 系统要解决什么问题？
- 核心用例是什么？
- 数据模型是什么？

#### 2. 非功能性需求（Non-Functional Requirements）
- **性能（Performance）**：QPS/TPS 要求？延迟要求？
- **可靠性（Reliability）**：容忍数据丢失吗？RPO/RTO 是多少？
- **可用性（Availability）**：允许多久的停机时间？
- **可扩展性（Scalability）**：数据量增长如何应对？
- **一致性（Consistency）**：强一致性还是最终一致性？

#### 3. 约束条件（Constraints）
- 单机还是分布式？
- 内存还是磁盘？
- 预算和团队规模？

**Redis 案例**：
```
功能需求：快速的键值存储
非功能需求：
  - 性能：100K+ QPS
  - 延迟：< 1ms
  - 可靠性：可选持久化
  - 可用性：通过主从复制保证
  - 一致性：最终一致性（复制延迟）
```

---

### 第二步：架构模式选择

**每个问题都有多种解决方案，关键是权衡 Trade-offs**

#### 核心架构决策树

```
┌─────────────────────────────────────────┐
│   需要持久化吗？                          │
├─────────────┬───────────────────────────┤
│ 是          │ 否                         │
│             │                            │
│ ┌──────────▼──────────┐    ┌───────────▼──────────┐
│ │ 快照 vs WAL？        │    │ 纯内存存储            │
│ │ RDB  vs AOF         │    │ (Memcached 模式)     │
│ └─────────────────────┘    └──────────────────────┘
│
│   需要高可用吗？
├─────────────┬───────────────────────────┤
│ 是          │ 否                         │
│             │                            │
│ ┌──────────▼──────────┐    ┌───────────▼──────────┐
│ │ 主从复制 + 故障转移  │    │ 单机部署              │
│ │ (Replication)       │    │                      │
│ └─────────────────────┘    └──────────────────────┘
│
│   需要水平扩展吗？
├─────────────┬───────────────────────────┤
│ 是          │ 否                         │
│             │                            │
│ ┌──────────▼──────────┐    ┌───────────▼──────────┐
│ │ 分片 (Sharding)      │    │ 垂直扩展 (Scale-up)  │
│ │ Redis Cluster       │    │                      │
│ └─────────────────────┘    └──────────────────────┘
└─────────────────────────────────────────┘
```

---

## 🏛️ Redis 架构演进路径

### Stage 0: 单机内存存储（已完成）

**架构模式**：单体架构（Monolithic）

```
┌──────────────────────────────────┐
│         Client                   │
└────────────┬─────────────────────┘
             │ TCP
┌────────────▼─────────────────────┐
│   Server (单进程/多线程)          │
│   ┌──────────────────────────┐   │
│   │  Protocol Layer (RESP)   │   │
│   ├──────────────────────────┤   │
│   │  Handler Layer           │   │
│   ├──────────────────────────┤   │
│   │  Storage Layer           │   │
│   │  map[string]interface{}  │   │
│   └──────────────────────────┘   │
└──────────────────────────────────┘
```

**设计决策**：
- ✅ **简单性**：架构清晰，易于理解和调试
- ✅ **性能**：无网络开销，内存访问极快
- ❌ **可靠性**：重启丢失所有数据
- ❌ **可用性**：单点故障，无容错能力
- ❌ **扩展性**：受限于单机内存

**Trade-offs**：牺牲可靠性和可用性，换取性能和简单性

**适用场景**：
- 缓存（数据可丢失）
- 会话存储（短期数据）
- 开发/测试环境

---

### Stage 1: 持久化架构（数据可靠性）

**架构模式**：Write-Ahead Logging + Snapshot

#### 模式 A：RDB 快照（Snapshot Pattern）

```
┌─────────────────────────────────────┐
│   In-Memory Storage                 │
│   ┌─────────────────────────────┐   │
│   │  Key-Value Store            │   │
│   └─────────────┬───────────────┘   │
│                 │                    │
│         ┌───────▼────────┐           │
│         │ Fork() 子进程   │           │
│         │ Copy-on-Write  │           │
│         └───────┬────────┘           │
│                 │                    │
│         ┌───────▼────────┐           │
│         │  dump.rdb      │           │
│         │  (磁盘快照)     │           │
│         └────────────────┘           │
└─────────────────────────────────────┘
```

**核心概念**：
- **快照（Snapshot）**：某一时刻的完整数据副本
- **Fork + COW**：子进程复制数据，不阻塞主进程
- **Point-in-Time Recovery**：恢复到特定时间点

**Trade-offs**：
- ✅ **优点**：
  - 恢复速度快（一次性加载）
  - 对性能影响小（后台保存）
  - 文件紧凑（二进制格式）
- ❌ **缺点**：
  - 可能丢失最后一次快照后的数据（RPO 较大）
  - Fork 时内存翻倍（COW 机制）
  - 大数据集保存耗时

**适用场景**：
- 全量备份
- 数据迁移
- 灾难恢复

---

#### 模式 B：AOF 日志（Write-Ahead Logging Pattern）

```
┌─────────────────────────────────────┐
│   Write Operation                   │
└──────────────┬──────────────────────┘
               │
       ┌───────▼────────┐
       │ 1. 写入 AOF 文件│
       │    (fsync)     │
       └───────┬────────┘
               │
       ┌───────▼────────┐
       │ 2. 修改内存数据 │
       └────────────────┘

AOF 文件格式：
*3\r\n$3\r\nSET\r\n$4\r\nname\r\n$5\r\nAlice\r\n
*2\r\n$3\r\nDEL\r\n$4\r\nname\r\n
...
```

**核心概念**：
- **Write-Ahead Log**：先写日志再写数据
- **日志回放（Replay）**：顺序执行日志恢复数据
- **日志压缩（Compaction）**：合并冗余命令

**Trade-offs**：
- ✅ **优点**：
  - 数据丢失少（可配置 fsync 频率）
  - 日志可读（便于分析和调试）
  - 支持部分恢复
- ❌ **缺点**：
  - 文件体积大（存储所有命令）
  - 恢复速度慢（逐条执行）
  - 需要定期重写

**fsync 策略对比**：

| 策略 | 性能 | 数据安全性 | 数据丢失风险 |
|------|------|-----------|-------------|
| `fsync always` | 慢 | 极高 | 0-1 条命令 |
| `fsync everysec` | 中 | 高 | 最多 1 秒数据 |
| `fsync no` | 快 | 低 | 取决于 OS |

**适用场景**：
- 金融系统（不能丢数据）
- 审计日志
- 事件溯源（Event Sourcing）

---

### Stage 2: 分布式架构（高可用）

**架构模式**：主从复制（Master-Slave Replication）

```
┌──────────────────────────────────────────────────┐
│                  Clients                         │
└─────────┬────────────────────────────┬───────────┘
          │ Write                      │ Read
┌─────────▼──────────┐        ┌────────▼──────────┐
│   Master (主节点)   │        │  Replica (从节点)  │
│   - 处理写请求      │────────│  - 处理读请求      │
│   - 记录复制日志    │ 复制   │  - 异步同步数据    │
└────────────────────┘        └────────┬──────────┘
                                       │
                              ┌────────▼──────────┐
                              │  Replica (从节点)  │
                              └───────────────────┘
```

**核心概念**：

#### 1. 复制协议（Replication Protocol）

```
Replica → Master: PSYNC <replication-id> <offset>
         ↓
Master 判断：
  - offset 在 backlog 中 → 增量同步
  - offset 过期或首次同步 → 全量同步

【全量同步】
Master → Replica: +FULLRESYNC <replication-id> <offset>
                  [RDB 快照数据]
                  [后续增量命令]

【增量同步】
Master → Replica: +CONTINUE
                  [backlog 中的增量命令]
```

#### 2. 复制偏移量（Replication Offset）

```
Master Offset:  12345
                  ↓ 复制延迟
Replica Offset: 12340

延迟 = Master Offset - Replica Offset
```

#### 3. 复制积压缓冲区（Backlog）

```
┌────────────────────────────────────┐
│   Replication Backlog (Ring Buffer)│
│                                    │
│   [CMD1][CMD2][CMD3]...[CMDN]     │
│      ↑                      ↑      │
│   oldest               newest      │
└────────────────────────────────────┘
```

**Trade-offs**：

| 复制模式 | 一致性 | 可用性 | 性能 | 复杂度 |
|---------|-------|-------|------|-------|
| **同步复制** | 强一致 | 低 | 慢（等待从节点） | 中 |
| **异步复制** | 最终一致 | 高 | 快（不等待） | 低 |
| **半同步复制** | 中等 | 中 | 中 | 高 |

**Redis 选择**：异步复制（优先性能和可用性）

**设计决策分析**：
- ✅ **读写分离**：Master 写，Replica 读（提升吞吐量）
- ✅ **数据冗余**：多副本存储（容错能力）
- ❌ **数据延迟**：异步复制导致短暂不一致
- ❌ **网络开销**：持续同步消耗带宽

---

### Stage 3: 自动故障转移（高可用升级）

**架构模式**：哨兵模式（Sentinel Pattern）

```
┌────────────────────────────────────────────────┐
│              Sentinel 集群                      │
│   ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│   │Sentinel 1│  │Sentinel 2│  │Sentinel 3│    │
│   └────┬─────┘  └────┬─────┘  └────┬─────┘    │
│        │             │             │           │
│        └─────────────┼─────────────┘           │
│               心跳检测 & 投票                    │
└────────────────┬───────────────────────────────┘
                 │
        ┌────────▼─────────────────────┐
        │  判断 Master 是否下线？        │
        ├──────────────────────────────┤
        │  主观下线（1 个 Sentinel）     │
        │  客观下线（多数 Sentinel 同意）│
        └────────┬─────────────────────┘
                 │
        ┌────────▼─────────────────────┐
        │  选举新的 Master              │
        ├──────────────────────────────┤
        │  1. 选择最佳 Replica          │
        │  2. 提升为 Master             │
        │  3. 通知其他节点              │
        └──────────────────────────────┘
```

**核心概念**：

#### 1. 故障检测（Failure Detection）
```
主观下线（SDOWN）：单个 Sentinel 认为节点下线
客观下线（ODOWN）：多数 Sentinel 同意节点下线（quorum）

心跳检测：
  - 间隔：每 1 秒 PING
  - 超时：30 秒无响应 → SDOWN
  - 投票：quorum 个 Sentinel 同意 → ODOWN
```

#### 2. 选主算法（Leader Election）
```
选择标准（优先级）：
1. replica-priority（手动配置的优先级）
2. replication offset（复制进度最新）
3. run_id（字典序最小）
```

**Trade-offs**：

| 方案 | 可用性 | 一致性 | 复杂度 | 成本 |
|------|-------|-------|-------|------|
| **手动故障转移** | 低（依赖人工） | 高 | 低 | 人力成本高 |
| **单 Sentinel** | 中（单点） | 中 | 低 | 资源少 |
| **Sentinel 集群** | 高（自动） | 中 | 高 | 资源多 |

**设计决策分析**：
- ✅ **自动化**：无需人工介入（减少 MTTR）
- ✅ **容错**：Sentinel 自身也是分布式（无单点）
- ❌ **脑裂风险**：网络分区可能产生多个 Master
- ❌ **复杂度**：需要管理额外的 Sentinel 集群

---

## 🔄 并发控制架构模式

### 模式 A：事务（Transaction Pattern）

**问题**：如何保证多个操作的原子性？

**Redis 方案**：乐观锁（WATCH + MULTI + EXEC）

```
┌──────────────────────────────────────┐
│  Client 1               Client 2     │
└────┬─────────────────────┬───────────┘
     │                     │
     │ WATCH balance       │
     │ (记录版本: v1)       │
     │                     │
     │                     │ SET balance 200
     │                     │ (版本变更: v1→v2)
     │                     │
     │ MULTI               │
     │ DECRBY balance 100  │
     │ EXEC                │
     │                     │
     ├─ 检查版本            │
     │  v2 ≠ v1            │
     │  → 事务失败          │
     └─────────────────────┘

返回：nil (事务失败)
```

**对比其他方案**：

| 方案 | 实现方式 | 并发度 | 性能 | 适用场景 |
|------|---------|-------|------|---------|
| **悲观锁** | 提前加锁 | 低 | 慢 | 冲突频繁 |
| **乐观锁** | 版本检查 | 高 | 快 | 冲突少 |
| **MVCC** | 多版本 | 极高 | 快 | 数据库 |

**Redis 选择**：乐观锁（适合高并发、低冲突场景）

---

### 模式 B：发布/订阅（Pub/Sub Pattern）

**问题**：如何实现进程间消息通信？

**架构**：观察者模式（Observer Pattern）

```
┌─────────────────────────────────────────┐
│          PubSub Broker                  │
│                                         │
│   channels = {                          │
│     "news": [client1, client2],         │
│     "chat": [client3],                  │
│   }                                     │
└──────┬──────────────────────┬───────────┘
       │                      │
   ┌───▼────┐            ┌────▼───┐
   │Publisher│           │Subscriber│
   │PUBLISH  │           │SUBSCRIBE │
   │news msg │           │news      │
   └─────────┘           └──────────┘
```

**对比其他消息模式**：

| 模式 | 消息持久化 | 消息顺序 | 消费模式 | 适用场景 |
|------|-----------|---------|---------|---------|
| **Redis Pub/Sub** | ❌ | ✅ | 广播 | 实时通知 |
| **Kafka** | ✅ | ✅ | 队列/广播 | 日志收集 |
| **RabbitMQ** | ✅ | ✅ | 队列 | 任务分发 |

**Trade-offs**：
- ✅ **低延迟**：内存转发，极快
- ✅ **简单**：无需 ACK，无持久化
- ❌ **不可靠**：订阅者离线丢消息
- ❌ **无回溯**：无法重新消费历史消息

---

## 🎯 架构设计原则总结

### 1. CAP 定理

```
┌──────────────────────────────────┐
│         CAP 定理                  │
│  只能同时满足两个：                 │
│                                  │
│  C - Consistency   (一致性)       │
│  A - Availability  (可用性)       │
│  P - Partition Tolerance (分区容错)│
└──────────────────────────────────┘

Redis 选择：AP（可用性 + 分区容错）
- 异步复制 → 最终一致性
- 故障转移 → 高可用性
```

### 2. 设计 Trade-offs 分析框架

**每个架构决策都问自己：**

| 维度 | 问题 | Redis 答案 |
|------|------|-----------|
| **性能 vs 可靠性** | 要速度还是安全？ | 可配置（AOF fsync） |
| **一致性 vs 可用性** | 要强一致还是高可用？ | 选择可用性（异步复制） |
| **简单性 vs 功能性** | 要简单还是强大？ | 保持简单（单线程） |
| **内存 vs 磁盘** | 快但贵 vs 慢但便宜？ | 内存优先 + 可选持久化 |

### 3. 可扩展性设计原则

```
垂直扩展（Scale-up）：
  ✅ 简单：加内存/CPU
  ❌ 限制：单机上限

水平扩展（Scale-out）：
  ✅ 无限：加机器
  ❌ 复杂：需要分片、路由

Redis 演进：
  阶段 1：单机（垂直）
  阶段 2：主从（读扩展）
  阶段 3：Cluster（分片写扩展）
```

---

## 📚 学习方法论

### 1. 先理解架构，再写代码

```
错误流程：
看文档 → 写代码 → 遇到问题 → 修改 → 勉强能用

正确流程：
分析需求 → 研究架构模式 → 画架构图 →
分析 Trade-offs → 选择方案 → 写代码 →
性能测试 → 优化
```

### 2. 每个 Phase 的学习重点

**不是**：实现功能、调通测试
**而是**：
- 这个架构模式解决什么问题？
- 为什么选择这个方案？
- 有哪些替代方案？
- Trade-offs 是什么？
- 如何应用到其他系统？

### 3. 文档写作要求

**每个 Phase 的设计文档应包含**：

```markdown
## Phase X: [名称]

### 问题定义
- 当前架构的局限性是什么？
- 要解决什么问题？

### 架构方案
- 选择什么架构模式？
- 为什么选择这个方案？
- 架构图（画图！）

### Trade-offs 分析
| 维度 | 方案A | 方案B | 我们的选择 | 原因 |
|------|------|------|-----------|------|
| 性能 | ... | ... | ... | ... |

### 关键设计决策
- 决策点 1：为什么这样设计？
- 决策点 2：考虑了哪些因素？

### 实现要点
- 核心算法
- 关键数据结构
- 需要注意的边界情况

### 验证方法
- 功能测试
- 性能基准测试
- 故障演练

### 学到了什么
- 架构模式：XXX Pattern
- 设计原则：XXX
- 可应用场景：XXX
```

---

## 🚀 下一步

现在，请阅读 **FOCUSED-ROADMAP.md**，它将以架构设计的视角重新规划学习路线。

**记住**：
- 不要追求功能完整性，而要追求架构理解深度
- 不要急于写代码，先理解为什么这样设计
- 不要孤立学习 Redis，而要学习通用的架构模式

**最终目标**：
- 能独立设计一个分布式系统
- 能分析和权衡不同架构方案
- 能将架构思维应用到任何系统
