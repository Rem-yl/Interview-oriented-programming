# Ch09 一致性与共识

由于分布式系统存在太多出错的场景，处理故障最简单的办怯就是直接让整个服务停下来，然后向用户提示出错信息。但**如果不能接受服务中止，就需要更加容错的解决方案**。

本章节讨论构建容错式分布式系统的相关算法和协议。

为了构建容错系统，最好先建立一套通用的抽象机制和与之对应的技术保证，这样只需实现一次，其上的各种应用程序都可以安全地信赖底层的保证。

> 这就像建房子时，先统一打好 “抗震地基”（通用抽象机制），并通过工程技术保证地基能抗 8 级地震（技术保证）。之后不管盖居民楼、办公楼还是商场（上层应用），都不用再自己设计抗震结构，只需直接建在这个地基上，就能安全应对地震。这样既省力，又比每个建筑各自设计抗震方案更可靠。

分布式系统最重要的抽象之一就是共识:**所有的节点就某项提议达成一致。**

> 分布式系统的核心矛盾是 “分散的节点” 与 “需要整体一致的行为” 之间的冲突。共识作为抽象，本质上是为了解决这种冲突—— 它屏蔽了底层网络、节点故障的复杂性，为上层提供了 “即使在不可靠环境中，所有节点仍能对关键决策达成一致” 的保证。没有共识，分布式系统就无法实现数据一致、协作可靠，更谈不上正确性和可用性。因此，共识成为分布式系统中最核心的抽象之一。

本章我们将主要研究解决共识问题的相关算法。

## 一致性保证

**最终一致性**: 如果停止更新数据库，并等待一段时间（长度未知）之后，最终所有读请求会返回相同的内容。

问题:

- "最终"可能很长时间
- 读取可能返回旧值
- 不同副本可能返回不同的值

本章主要探讨的内容:

- 最强的一致性模型: **线性化**
- 探讨分布式系统中事件顺序问题, 特别是**因果关系和全局顺序**
- 探索如何自动提交分布式事务, 并最终解决**共识问题**

## 可线性化

**直觉含义**: 让一个系统看起来好像只有一个数据副本, 且所有的操作都是原子的。

通过记录所有请求和响应的顺序，然后检查它们是否可以顺序排列，可以用来测试系统是否可线性化

**核心特征**:

1. **原子性**: 一旦写入完成,所有客户端立即看到新值
2. **全局顺序**: 所有操作有唯一的全局时间顺序
3. **实时性**: 如果操作A在操作B开始前完成,A必须在B之前

**可线性化与可串行化的区别**

可线性化：强绑定真实时间。

假设操作 A 在真实时间 t1 完成，操作 B 在真实时间 t2（t2>t1）开始，那么在可线性化的系统中，A 的执行顺序必须排在 B 之前。例：用户 1 在 10:00:01 完成 “给计数器 + 1”（结果从 0→1），用户 2 在 10:00:02 开始 “读计数器”，那么用户 2 必须读到 1，不能读到0。

可串行化：不绑定真实时间。

只要多个事务的最终结果等价于 “某种串行顺序” 即可，不管事务中操作的真实发生时间。
例：事务 T1（A 转 100 给 B）和事务 T2（B 转 100 给 C）并发执行：

- 真实时间顺序可能是 “T1 读 A→T2 读 B→T1 写 A→T2 写 B→T1 读 B→T1 写 B→T2 读 C→T2 写 C”；
- 但最终结果只要是 “A-100、B 不变、C+100”，就等价于 “先 T1 再 T2” 或 “先 T2 再 T1” 的串行结果，满足可串行化 —— 哪怕中间操作的真实顺序打乱了。

## 线性化的依赖条件

### 加锁与主节点选举

主从复制的系统需要确保有且只有一个主节点，否则会产生脑裂。

选举新的主节点常见方法是使用锁：即**每个启动的节点都试图获得锁，其中只有一个可以成功成为主节点。**

锁的实现必须满足可线性化：**所有节点都必须同意那个节点持有锁。**

归根结底，线性化存储服务是所有这些协调服务的基础。

### 约束与唯一性保证

硬性的唯一性约束，常见的如关系型数据库中的主键的约束，则需要线性化保证；其他如外键或者属性约束，则并不要求一定线性化。

> 主键的唯一性是 “不能有任何冲突” 的硬性要求，必须通过线性化保证操作的严格顺序和结果可见性；而外键、属性约束更关注 “最终是否符合规则”，对并发操作的实时顺序要求较低，因此不需要线性化。

### 跨通道的时间依赖

线性化违例之所以被注意到，是因为系统中存在其他的通信渠道。

> 简单说：系统自己不会喊 “我违例了”，但用户之间的聊天、系统之间的日志比对等 “额外沟通”，会让 “你看到的和我看到的不一样” 这个事实浮出水面 —— 这就是这句话的含义。

## 实现线性化系统

**最简单的方案：只用一个数据副本。**

但是该方案无法容错，如果该副本节点发生故障就会导致数据丢失。

**系统容错最常见的方怯就是采用复制机制。**

- 主从复制(部分支持可线性化)
- 共识算法(可线性化)
- 多主复制(不可线性化)
- 无主复制(可能不可线性化)

### 线性化与quorum

在分布式系统中，quorum（通常翻译为 “仲裁” 或 “法定人数”） 是一种通过 “节点数量约定” 来平衡数据一致性和系统可用性的机制。它的核心思想是：**通过设定一个 “最小节点数阈值”，只有当操作（读 / 写）获得至少这么多节点的响应时，才认为操作成功，从而避免因部分节点故障或网络分区导致的数据混乱。**

线性化（Linearizability）和 quorum（仲裁）是分布式系统中两个关联密切但目标不同的概念：quorum 是一种通过 “节点数量约定” 平衡可用性与一致性的机制，而线性化是一种强一致性保障（要求操作顺序符合真实时间）。两者的关系可以概括为：quorum 可以作为实现线性化的工具，但单纯的 quorum 机制并不一定能保证线性化，需要额外条件配合。

## 线性化的代价

![img](../img/网络中断.png)

如果应用程序要求线性化读写，则网络中断一定会违背这样的要求。

### CAP理论

**CAP 定理** (也称为 Brewer's Theorem) 指出:在一个分布式系统中,以下三个属性**不可能同时满足**,最多只能同时满足其中两个:

- **C (Consistency)** - 一致性
- **A (Availability)** - 可用性
- **P (Partition Tolerance)** - 分区容错性

更额外的是，由于分布式系统不可避免的存在网络分区，P(分区容错性)必须得到满足，因此剩下的就是在C或者A中选择。

在网络正常的时候，系统可以同时保证一致性（线性化）和可用性。而一旦发生
了网络故障，必须要么选择线性（一致性），要么可用性。

正式定义的CAP定理范围很窄，它只考虑了一种一致性模型（即线性化）和一种故障（网络分区，节点仍处于活动状态但相互断开），而没有考虑网络延迟、节点失败或其他需要折中的情况。因此对于现代的系统设计来说，没有太多实际的价值。

### 可线性化与网络延迟

线性化对性能的影响是巨大的，例如在现代多核CPU系统中的内存就是非线性化的，为了速度考虑。

## 顺序保证

由于线性一致性对面表现的就像是只有一个数据副本，每个操作都是原子性生效的，这就意味着操作对外是按照某种顺序执行的。

## 顺序与因果关系

顺序有助于保持因果关系，而**因果关系的重要性核心在于：它是维持 “操作逻辑正确性” 和 “系统行为可预测性” 的基石—— 如果忽略因果关系，原本符合直觉的操作会变得混乱，甚至导致数据错误、业务逻辑崩坏。**

一旦因果关系被破坏，系统会从 “可预测的可靠工具” 变成 “充满不确定性的黑盒”，即违背了人类的直觉。因果关系的依赖链条定义了系统中的因果顺序，即某件事应该发生另一件事情之前。

如果系统服从因果关系所规定的顺序，我们称之为因果一致性。

### 因果顺序并非全序

全序关系支持任何两个元素之间进行比较，例如对自然数比大小。如果两个事件是因果关系（一个发生在另一个之前），那么这两个事件可以被排序；而井发的事件则无法排序比较。这表明因果关系至少可以定义为偏序，而非全序。

> 因果关系只能确定有依赖的事件之间的先后（偏序），但无法对所有事件（包括并发事件）排序（全序）。

并发意味着时间线会出现分支和合并，在不同分支上的操作无法直接比较。

### 可线性化强于因果一致性

可线性化与因果序之间的关系：**可线性化一定意味着因果关系：任何可线性化的系统都将正确地保证因果关系。**

但是可线性化会显著降低性能和可用性，尤其是在严重网络延迟的情况下。

在许多情况下，许多看似需要线性化的系统实际上真正需要的是因果一致性，后者的实现可以高效很多。

> 线性化并非是保证因果关系的唯一途径，还有其他方法可以使系统满足因果一致性而避免性能问题。

### 捕获因果依赖关系

本节简要探讨非线性化系统如何保证因果一致性。

为了确定因果关系，数据库需要知道应用程序读取的是哪个版本的数据。

## 序列号排序

显示跟踪所有已读数据意味着巨大的开销；**我们可以使用序列号或者时间戳来排序事件。** 这样的序列号或者时间戳非常的紧凑，每一个操作都有唯一的序列号，并且总是可以通过比较来确定谁大。

在主从复制的数据库中，主节点可以简单的为每个操作递增某个计数器，从而为复制日志中的每个赋值操作分配一个递增的序列号。

### 非因果序列发生器

本节讨论不存在唯一主节点的系统如何产生序列号。

- 每个节点都独立产生自己的一组序列号;
- 可以把墙上时钟的时间戳附加到每个操作上(即物理时间);
- 可以预先分配序列号的范围;

以上三个思路都可行，但是他们 **所产生的序列号与因果关系并不严格一致。** 所有这些序列号发生器都无法保证正确捕获跨节点操作的顺序，因而存在因果关系方面的问题

### Lamport时间戳

Lamport 时间戳（Lamport Timestamp）是分布式系统中用于给事件（如节点的读写操作、消息发送）分配全局序列号的算法，由 Leslie Lamport 提出。它的核心作用是：**在没有全局统一时钟的分布式环境中，通过一套简单的规则，让不同节点产生的事件能够被 “逻辑排序”，尤其能正确反映事件之间的因果关系**（如 “A 事件导致 B 事件”，则 A 的时间戳一定小于 B）。

分布式系统的节点各自有本地时钟，但由于网络延迟、时钟漂移等问题，不同节点的物理时间无法完全同步；如果直接使用本地时间来给事件排序则可能会出现错误。

**Lamport 时间戳的生成规则（核心逻辑）**
每个节点维护一个本地计数器（counter），初始值为 0。当节点发生事件（自身操作或接收消息）时，按以下规则更新计数器，生成时间戳：

- 规则 1：节点自身发生事件（如本地写操作）
  先将本地计数器加 1（counter = counter + 1），然后用这个新值作为该事件的 Lamport 时间戳。
- 规则 2：节点发送消息
  先按规则 1 更新本地计数器（生成时间戳 T），然后将消息和 T 一起发送给其他节点。
- 规则 3：节点接收消息
  收到消息后，先比较 “消息携带的时间戳 T_msg” 和 “本地当前计数器 counter”，取两者中的最大值加 1，作为本地计数器的新值（counter = max(T_msg, counter) + 1），再用这个新值作为 “接收消息” 这个事件的时间戳。

Lamport时间戳与物理墙上时钟并不存在直接对应关系，但它可以保证全序：**给定两个Lamport时间戳，计数器较大那个时间戳大；如计数器值正好相同，则节点I D 越大，时间戳越大。**

### 时间戳排序依旧不够

虽然Lamport时间戳定义了全序关系，但是还是不足以解决分布式系统中的很多常见问题。例如：全局一致性用户ID问题。

这个问题的根本是：当节点创建用户ID时，他们不知道另一个节点是否也正在同时创建用户（以及请求所带的时间戳），如果该节点出现故障或者网络延迟问题，则时间戳方法就无法正常运转。

总而言之，为了实现像用户名唯一性约束这样的目标，仅仅对操作进行全序排列还是不够的，还需要知道这些操作是否发生、何时确定等。

## 全序关系广播

前一节讨论的按照时间戳或者序列号排序，发现它不如**主从复制**那么直接有效。

这一节主要讨论的问题:

1. 如何扩展系统的吞吐量使其突破单一主节点的限制;
2. 如何处理主节点失效时的故障切换

这些问题被称为**全序关系广播**或者**原子广播**

全序关系广播通常指节点之间交换消息的某种协议，其必须要满足以下两点安全属性:

1. 可靠发送。没有消息丢失;
2. 严格有序。消息总是以相同顺序发送到每个节点。

### 使用全序关系广播

**什么是"全序关系广播"**

全序关系广播是一种消息传递机制，核心特点是：

- 多个节点之间传递消息时，所有节点会以完全相同的顺序接收和处理消息（比如节点 A 收到消息的顺序是 M1→M2→M3，节点 B、C 也必须是 M1→M2→M3）。
- 消息一旦被确定顺序，就不允许 “插队”（不能事后把一条新消息插入到之前的顺序中），类似日志的 “追加不可变” 特性（只能往后加，不能改前面）。

**全序关系广播与共识服务（如 ZooKeeper、etcd）的关系**

ZooKeeper、etcd 这类共识服务的核心能力之一，就是实现了全序关系广播。

1. 共识（Consensus）的目标是：让分布式节点在 “哪个值被选中”“按什么顺序执行” 等问题上达成一致。
2. 全序关系广播是达成共识的关键手段：通过保证 “所有节点以相同顺序处理消息”，自然就能让节点对 “操作顺序” 达成共识（比如 “先执行哪个写请求，后执行哪个”）。

因此，两者密切相关：全序关系广播是共识服务的核心功能，共识服务通过全序广播实现了节点间的操作顺序一致。

**全序关系广播的核心应用场景**
(1) 数据库复制：保证副本一致性（状态机复制）

数据库通常会有多个副本（如主从复制），要让所有副本数据一致，关键是 **“所有副本按相同顺序处理写请求”**。

1. 每条写请求（如 “更新用户余额”）作为一条消息，通过全序广播发送给所有副本。
2. 无论副本位于哪个节点，都严格按消息的全局顺序执行写操作（比如先执行 M1 “张三充值 100”，再执行 M2 “张三消费 50”），最终所有副本的状态会完全一致（即使某些副本处理稍慢，滞后一点，最终也会追上）。
3. 这种机制被称为 “状态机复制”：把每个副本看作一个 “状态机”，只要输入（写请求）的顺序相同，初始状态相同，最终状态就一定相同。

(2) 可串行化事务：保证分布式事务一致性

可串行化要求 “多个事务的执行结果等价于按某一顺序逐个执行”。全序广播可以实现这一点：

1. 每个事务（如 “转账 + 扣库存”）作为一条消息，通过全序广播确定全局顺序。
2. 所有节点（或数据库分区）按相同的顺序执行这些事务（比如 T1→T2→T3），即使事务在分布式环境中并发发起，最终结果也和 “先执行 T1，再执行 T2，最后执行 T3” 完全一致，避免了分布式事务的冲突。

(3) 分布式锁与令牌：生成单调递增序列号
全序广播的 “消息顺序不可变” 特性，适合生成全局唯一、单调递增的序列号（如分布式锁的令牌）

简单说：全序关系广播就像给分布式系统的所有操作 “统一排号”，所有节点必须按这个号码顺序处理，因此无论在哪个节点，最终结果都能保持一致。

### 采用全序关系广播实现线性化存储
使用全序关系广播可以实现写操作的线性化，但是读操作还是略微要复杂一些；可以使用以下方法来实现读操作的线性化：
1. 读操作也通过全序广播;
2. 从已经同步完成的副本读;
3. 只从主节点读

### 采用线性化存储实现全序关系广播
当我们有线性化的存储，我们可以在其上构建全序关系广播。

最简单的方法是假设有一个线性化的寄存器来存储一个计数，然后使其支持原子自增-读取操作或者原子-比较设置操作。

算法思路很简单：对于每个要通过全序关系广播的消息，原子递增井读取该线性化的计数，然后将其作为序列号附加到消息中。接下来，将消息广播到所有节点（如果发生丢失，则重新发送），而接受者也严格按照序列化来发送回复消息。

作者在最后指出：**线性化的原子比较设置（或自增）寄存器与全序关系广播二者都等价于共识问题。**

## 分布式事务与共识
共识问题是分布式计算中最重要也是最基本的问题之一，其基本目标：**让分布式系统中的所有节点在某件事上打成一致。**

有许多场景需要集群节点达成共识:
- 主节点选举
- 原子事务提交
    所有节点必须在事务的结果上达成一致：要么全部成功提交，要么终止/回滚

> 共识的不可能性: 根据FLP结论，如果节点存在可能崩溃的风险，那么不可能存在总是能够达成共识的稳定算法。但是我们可以使用超时或者其他方法来检测崩溃节点，从而实现稳定的共识算法。

## 原子提交与两阶段提交

事务原子性可以为上层应用提供非常简单的语义：事务的结果要么是成功提交的，要么是中止的。防止失败的事务破坏系统。

### 从单节点到分布式的原子提交

对于在单个数据库节点上执行的事务，原子性通常由存储引擎来负责。

当事情来到多个节点上执行事务，情况就变得复杂起来。向所有节点简单地发送一个提交请求，然后各个节点独立执行事务提交是绝对不够的。这样做很容易发生部分节点提交成功，而其他一些节点发生失败。

因此在多节点上，事务提交不可撤销，也不能事后再改变主意。因为一旦数据提交，就会被其他事务可见，其他客户端就会基于此做出相应的决策。已提交的事务可以被之后的一笔新事务抵消，即补偿性事务。

### 两阶段提交(two-phase commit, 2PC)

两阶段提交（Two-Phase Commit，2PC）是一种分布式事务一致性协议，核心目标是保证分布式系统中多个参与者（如数据库、微服务）对事务的操作满足原子性。

**主要内容**

1. 核心角色
    - 协调者（Coordinator）：负责发起、管理整个分布式事务的流程，最终决定事务是 “提交” 还是 “回滚”（类似 “事务管理器”）。
    - 参与者（Participant）：实际执行事务分支操作的节点（如不同数据库、微服务），每个参与者管理自身的本地事务资源（如数据、锁）。

2. 协议流程
    将事务提交过程分为 **“投票阶段（准备阶段）”和“提交 / 回滚阶段”**，通过两阶段的交互确保全局一致性。

**详细步骤**

第一阶段：投票阶段（准备阶段）

1. 发起事务与执行本地操作
    协调者发起分布式事务后，向所有参与者发送指令，要求它们执行本地事务分支操作（如执行 SQL、锁定资源），但不提交事务（即操作已执行，但变更暂不生效）。
2. 协调者发起 “准备提交” 询问
    协调者向每个参与者发送 Prepare（或 Can Commit?）请求，询问：“你是否准备好提交事务？本地操作执行成功了吗？”
3. 参与者投票响应
    - 若参与者成功执行本地操作（如 SQL 无错误、资源已锁定、日志已落盘），则回复 Yes（同意提交），并进入阻塞状态（保留资源 / 锁，等待协调者最终指令）。
    - 若参与者执行本地操作失败（如违反约束、资源不足、超时等），则回复 No（不同意提交），同样进入阻塞状态（后续会回滚释放资源）。

第二阶段：提交 / 回滚阶段（根据投票结果决策）

协调者根据第一阶段收集到的所有参与者的投票，做出全局提交或全局回滚的决定：

- 情况 1：所有参与者都回复 Yes（全员准备好提交）：
    1. 协调者向所有参与者发送 Commit（提交）指令。
    2. 参与者收到 Commit 后，正式提交本地事务（使变更永久生效），然后释放持有的资源 / 锁，并向协调者发送确认（ack）。
    3. 调者收到所有参与者的 ack 后，确认整个分布式事务全局提交成功。

- 情况 2：至少一个参与者回复 No（或超时未响应）（存在参与者无法提交）：
    1. 协调者向所有参与者发送 Rollback（回滚）指令。
    2. 参与者收到 Rollback 后，回滚本地事务（撤销所有已执行但未提交的操作），释放资源 / 锁，并向协调者发送确认（ack）。
    3. 协调者收到所有参与者的 ack 后，确认整个分布式事务全局回滚。

可以简单类比成现实生活中的结婚仪式：主持人会向双方确认是否愿意结婚，当有任意一方不愿意，则婚礼仪式应当终止。

### 系统的承诺

通过上述对2PC流程的描述，自然而然引出了一个疑惑：**对于2PC，准备和提交请求也一样可能会发生丢失**，那么2PC有何不同之处？

**2PC应对消息丢失的关键设计**

1. 第一阶段（准备阶段）：通过 “超时重试” 避免 “准备请求丢失” 导致的不一致

    - 场景 1：协调者发送的 “准备请求” 丢失协调者向参与者发送 Prepare 请求后，若长时间未收到响应（超时），会默认该参与者 “无法准备”，直接判定全局事务需要回滚，并向所有参与者发送 Rollback 指令。此时，未收到 Prepare 请求的参与者从未执行任何操作，回滚指令对它无影响；已收到并响应 Yes 的参与者会执行回滚 —— 最终所有参与者状态一致（都未提交）。

    - 场景 2：参与者的 “准备响应（Yes/No）” 丢失参与者成功执行本地操作并回复 Yes 或 No，但响应消息丢失，导致协调者超时。协调者会按 “超时 = 失败” 处理，发起全局回滚。此时，已回复 Yes 的参与者虽然在等待最终指令，但收到 Rollback 后会回滚本地操作；未收到响应的协调者决策也不会影响一致性 —— 最终仍保持全回滚。
    
2. 第二阶段（提交 / 回滚阶段）：通过 “持久化日志 + 重试” 确保 “最终决策被执行”

    2PC 的核心保障在于：一旦协调者做出 “提交” 或 “回滚” 的全局决策，会通过持久化和重试确保所有参与者最终执行该决策，即使中间消息丢失。

    - 关键前提：决策的持久化

        协调者在发送 Commit 或 Rollback 指令前，会将决策结果（“全局提交” 或 “全局回滚”）写入本地持久化日志（如磁盘文件）。这意味着：即使协调者崩溃重启，也能从日志中恢复之前的决策，继续向未收到指令的参与者发送。

    - 场景 1：“提交指令” 丢失

        协调者已决定提交并写入日志，但向部分参与者发送的 Commit 指令丢失。

        - 参与者会一直处于 “准备状态”（持有资源锁，等待最终指令）。
        - 协调者发现超时后，会重新发送 Commit 指令（直到参与者确认收到）。
        - 参与者收到 Commit 后，基于本地准备阶段的持久化日志（已记录事务操作），执行提交并释放资源 —— 最终所有参与者都提交。

    - 场景 2：“回滚指令” 丢失
        协调者决定回滚并写入日志，但 Rollback 指令丢失。
        - 协调者超时后会重试发送 Rollback 指令。
        - 参与者收到后，基于本地日志回滚操作，释放资源 —— 最终所有参与者都回滚。
3. 参与者崩溃后的恢复：依赖 “持久化日志” 找回状态

    若参与者在 “准备阶段” 或 “等待最终指令” 时崩溃，重启后会通过读取本地持久化日志恢复状态：

    - 若日志显示 “已响应 Yes 但未收到最终指令”，参与者会保持阻塞，向协调者发送 “状态查询” 请求。
    - 协调者根据自身持久化的决策日志，回复 Commit 或 Rollback，参与者按指令执行 —— 确保崩溃后仍能与全局状态一致。

本质上就是，2PC通过阻塞/超时等待等方法保证事务的原子性; 而多节点独立提交事务由于缺乏**协调者**，节点之间无法判断对方的状态。

### 协调者发生故障

当所有的参与者都已经完成了投票，协调者发生崩溃时，由于参与者不能**单方面放弃**，只能等待协调者的决定，那么参与者就无法进行下一步行动。

如何解决协调者故障问题不是2PC协议的范畴。2PC能够顺利完成的唯一方法是等待协调者恢复。

### 三阶段提交

两阶段提交也被称为阻塞式原子提交协议，因为2PC可能在等待协调者恢复时卡住。理论上，可以使其改进为非阻塞式从而避免这种情况。但是，实践中要想做到这一点并不容易。

现实中也有3PC方案替代2PC，但他仍然无法保证原子性。因此尽管存在协调者崩溃等等问题，但普遍情况下还是使用2PC协议。

## 实践中的分布式事务

2PC协议存在各种性能上的问题，出现性能下降的主要原因是**为了防崩溃恢复而做的磁盘I/O(fsync)以及额外的网络往返开销。**

分布式事务的概念:
1. 数据库内部的分布式事务
    所有参与节点都运行着相同的数据库软件

2. 异构分布式事务
    在异构分布式事务中，存在两种或两种以上不同的参与者实现技术

### Exactly-once消息处理

异构的分布式事务旨在无缝集成多种不同的系统。

Exactly-once 消息处理：确保消息 “不多不少” 被处理一次

在分布式系统中，消息传递（如 Kafka、RabbitMQ 等消息队列）可能因网络重试、节点崩溃等问题出现 “重复投递” 或 “丢失”，导致：

- At-most-once（最多一次）：消息可能丢失，处理 0 次或 1 次（比如发送后不重试，丢了就丢了）；
- At-least-once（至少一次）：消息不会丢失，但可能重复（比如发送后超时重试，导致接收方收到多次）。

而 Exactly-once（精确一次） 是更严格的目标：**无论中间出现什么故障，消息最终只被处理一次（不丢、不重）。**


### XA交易

“异构分布式事务” 指的是：事务涉及的参与者是不同类型的系统（如 MySQL 数据库、PostgreSQL 数据库、消息队列、缓存等），需要保证这些系统的操作 “要么全成功，要么全失败”。

**XA 协议（由 X/Open 组织定义）是实现这种异构事务的标准化接口**，它规定了 “协调者” 和 “参与者” 之间的通信规则，让不同厂商的系统能无缝协作。

### 停顿时仍持有锁

由于数据库事务通常持有待修改行的独占锁，在事务提交/中断之前都不会释放这些锁。因此在2PC提交时，事务在整个协调者崩溃期间都会锁定修改对象。

数据加锁时，其他事务就无法执行修改，因此必须解决处于停顿状态的事务。

### 从协调者故障中恢复

当协调者崩溃并且日志丢失或者损坏，这时就需要管理员手动决定是执行事务的回滚还是提交。

### 分布式事务的限制

这节主要讨论XA协议在实际应用的局限性:
1. 协调者的单点故障;
2. 破坏“应用服务器无状态部署”的设计模式
    协调者的日志会成为 “可靠系统的关键组件”（需用它恢复有疑问的事务），这让应用服务器从 “无状态” 被迫变成 “有状态”，彻底改变了部署的灵活性。
3. 兼容性导致的最低标准困境;
4. 扩大事务失败的风险
    只要有一个参与者故障或投票反对，整个分布式事务就会失败，与构建容错系统目标相违背

## 支持容错的共识

共识问题通常形式化描述如下：一个或多个节点可以提议某些值，由共识算法来决定最终值。

共识算法必须满足以下性质:
1. 协商一致性；所有节点都接受相同的决议
2. 诚实性；所有节点不能反悔，对同一项提议不能有两次决定
3. 合法性；如果决定了值v，则这个值一定是某个节点提议的
4. 可终止性；节点不崩溃则最终一定可以达成决议

前三个属性可以通过“独裁节点”来满足，即所有的决议都由主节点来决定；但是这就会导致主节点失效时的单点故障问题。可终止性引入了容错的思想，即某个节点出现了故障，其他节点也必须做出决定，不能让系统空转。

算法能够容忍的失败次数和规模都由一定的限制（所有节点都崩溃了，算法就不可能做出决定）。一般的，可终止性的前提是：**发生崩溃或者不可用的节点数必须小于半数节点。**

### 共识算法与全序广播

著名的容错式共识算陆包括VSR, Paxos, Raft和Zab。

这些算法大部分其实并不是直接使用上述的形式化模型（提议井决定某个值，同时满足上面4个属性）。相反，他们是决定了一系列值，然后采用全序关系广播算法。

全序关系广播的要点是，消息按照相同的顺序发送到所有节点，有且只有一次。这其实相当于进行了多轮的共识过程：在每一轮，节点提出他们接下来想要发送的消息，然后决定下一个消息的全局顺序。

### 主从复制与共识

主从复制需要解决主节点选举问题，我们需要通过共识算法来选择一个主节点，但是在选举主节点之前我们需要一个主节点，emmm，陷入了一个美妙的循环。

### 共识的局限性

1. 在达成一致性决议之前，节点投票的过程是一个同步复制过程。
2. 共识体系需要严格的多数节点才能运行。
3. 多数共识算在是假定一组固定参与投票的节点集，这意味着不能动态、添加或删除节点。
4. 共识系统通常依靠超时机制来检测节点失效。
    在网络延迟高不确定的情况下，系统可能会花费更多的时间在主节点选举而不是原本的服务任务上