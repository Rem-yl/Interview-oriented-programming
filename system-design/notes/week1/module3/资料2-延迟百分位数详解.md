# 延迟百分位数 (Latency Percentiles) 详解

> 为什么 P99 比平均值更重要？

---

## 一、为什么平均值不够？

### 1.1 平均值的陷阱

**案例**: 某 Web API 的响应时间统计

```
100 次请求的响应时间:
- 前 90 次: 10ms
- 第 91-95 次: 50ms
- 第 96-98 次: 200ms
- 第 99 次: 500ms
- 第 100 次: 5000ms (超时)

平均值计算:
= (10 × 90 + 50 × 5 + 200 × 3 + 500 + 5000) / 100
= (900 + 250 + 600 + 500 + 5000) / 100
= 7250 / 100
= 72.5ms

看起来很不错？但实际上:
- 10% 的用户体验到 >50ms 延迟
- 5% 的用户体验到 >200ms 延迟
- 1% 的用户等待了 5 秒！
```

**问题**:
1. **平均值掩盖了长尾**: 少数极慢的请求被大量快速请求"平均掉"
2. **无法反映用户体验**: 1% 的用户受影响 = 日活百万就是 1 万用户流失
3. **无法定位问题**: 不知道有多少用户受影响

### 1.2 真实案例: Netflix 的教训

**背景**: Netflix 曾用平均响应时间作为 SLA 指标。

**问题**:
```
系统 A: 平均延迟 100ms
- 99% 请求 < 50ms
- 1% 请求 > 10s (严重超时)

系统 B: 平均延迟 150ms
- 100% 请求在 100-200ms 之间

从平均值看，系统 A 更好 (100ms < 150ms)
但从用户体验看，系统 B 更稳定！
```

**教训**: Netflix 改用 P99.9 延迟作为核心指标，大幅提升了用户体验。

---

## 二、百分位数 (Percentiles)

### 2.1 定义

**Percentile (百分位数)**: 在一组数据中，有 X% 的数据小于或等于该值。

**常用百分位数**:

```
P50 (中位数, Median):
- 50% 的请求延迟 ≤ 该值
- 代表"典型用户体验"

P90:
- 90% 的请求延迟 ≤ 该值
- 10% 的请求更慢

P95:
- 95% 的请求延迟 ≤ 该值
- 5% 的请求更慢

P99:
- 99% 的请求延迟 ≤ 该值
- 1% 的请求更慢
- 代表"最差用户体验"

P99.9:
- 99.9% 的请求延迟 ≤ 该值
- 0.1% 的请求更慢
- 极少数异常情况
```

### 2.2 手工计算

**数据集**: 10 个样本 (已排序)

```
样本: [5, 10, 15, 20, 25, 30, 35, 40, 45, 50] ms

P50 计算:
位置 = 10 × 50% = 5
P50 = 第 5 个值 = 25ms

P90 计算:
位置 = 10 × 90% = 9
P90 = 第 9 个值 = 45ms

P95 计算:
位置 = 10 × 95% = 9.5
P95 = (第 9 个 + 第 10 个) / 2 = (45 + 50) / 2 = 47.5ms
```

**大数据集**: 100 个样本

```
样本: [1, 2, 3, ..., 100] ms (已排序)

P50 = 第 50 个值 = 50ms
P90 = 第 90 个值 = 90ms
P95 = 第 95 个值 = 95ms
P99 = 第 99 个值 = 99ms
```

### 2.3 实际计算方法

**方法 1: 排序法** (精确但慢)

```python
import numpy as np

latencies = [10, 50, 20, 30, 100, 15, 25, 200, 35, 500]

p50 = np.percentile(latencies, 50)  # 27.5ms
p95 = np.percentile(latencies, 95)  # 200ms
p99 = np.percentile(latencies, 99)  # 440ms

print(f"P50: {p50}ms, P95: {p95}ms, P99: {p99}ms")
```

**方法 2: 近似算法** (快但近似)

```
t-digest 算法 (Prometheus 使用):
- 不需要完整排序
- 流式计算
- 内存占用 O(1)
- 误差 < 1%

HDRHistogram (高精度):
- 固定内存
- 亚秒级查询
- 适合实时监控
```

---

## 三、为什么 P99 如此重要？

### 3.1 业务影响

**用户流失率**:

```
假设:
- 日活用户 (DAU): 1,000,000
- P99 延迟 > 3s (用户体验差)

影响用户数 = 1,000,000 × 1% = 10,000 人/天

如果 10% 流失:
- 每天流失 1,000 用户
- 每月流失 30,000 用户
- 年损失 = 30,000 × 12 × LTV (用户终身价值)
```

**收入影响**:

```
Amazon 研究:
- 延迟增加 100ms → 销售额下降 1%

Google 研究:
- 搜索延迟增加 500ms → 流量下降 20%

示例计算:
年营收 $100M 的电商平台
P99 延迟从 500ms 提升到 200ms (-300ms)
营收增加 = $100M × 3% = $3M/年
```

### 3.2 尾延迟放大效应 (Tail Latency Amplification)

**单服务场景**:

```
服务 A:
- P99 = 100ms
- 1% 的请求 > 100ms

看起来还不错？
```

**微服务场景** (关键问题!):

```
用户请求需要调用 10 个下游服务 (串行):

每个服务 P99 = 100ms (即 1% 概率超时)

至少有 1 个服务超过 P99 的概率:
= 1 - (1 - 0.01)^10
= 1 - 0.99^10
= 1 - 0.904
= 0.096 ≈ 9.6%

结论:
- 单个服务的 P99 (100ms) ≈ 整体的 P90！
- 用户请求的 P99 会远大于 100ms
```

**可视化**:

```
调用链长度 | 至少1个服务P99超时的概率 | 整体延迟百分位
-----------|-------------------------|------------------
1 个服务   | 1%                      | P99
5 个服务   | 4.9%                    | P95
10 个服务  | 9.6%                    | P90
50 个服务  | 39.5%                   | P60
100 个服务 | 63.4%                   | P37

教训: 微服务越多，尾延迟影响越大！
```

### 3.3 SLA 设计

**反面案例** (只看平均值):

```yaml
SLA (错误):
  - metric: avg_latency
    threshold: < 100ms

问题:
- 平均 50ms，但 P99 可能是 5s
- 1% 用户体验极差，但 SLA 达标
```

**正确案例** (基于百分位数):

```yaml
SLA (正确):
  - metric: p50_latency
    threshold: < 20ms    # 普通用户体验

  - metric: p95_latency
    threshold: < 100ms   # 95% 用户可接受

  - metric: p99_latency
    threshold: < 500ms   # 避免极端差体验

  - metric: p99.9_latency
    threshold: < 2s      # 容忍极少数超时
```

**实际案例: Google Search**:

```
Google 内部 SLA:
- P50: < 50ms
- P95: < 200ms
- P99: < 500ms
- P99.9: < 1s

为什么这么严格?
- 搜索是核心业务
- 用户对延迟极敏感
- 微服务调用链长
```

---

## 四、测量与监控

### 4.1 Histogram (直方图)

**原理**: 将延迟分桶统计

```
桶定义:
[0-10ms]: ████████ 40%
[10-50ms]: ████████████████ 35%
[50-100ms]: ████████ 15%
[100-200ms]: ████ 8%
[200-500ms]: ██ 1.5%
[500ms+]: █ 0.5%

从直方图可快速估算:
P50 ≈ 10-50ms 区间
P95 ≈ 100-200ms 区间
P99 ≈ 200-500ms 区间
```

**Prometheus 配置**:

```go
import "github.com/prometheus/client_golang/prometheus"

var requestDuration = prometheus.NewHistogram(
    prometheus.HistogramOpts{
        Name: "http_request_duration_seconds",
        Help: "HTTP request latencies",
        Buckets: []float64{
            0.001, // 1ms
            0.005, // 5ms
            0.01,  // 10ms
            0.05,  // 50ms
            0.1,   // 100ms
            0.5,   // 500ms
            1.0,   // 1s
            2.0,   // 2s
            5.0,   // 5s
        },
    },
)

// 使用
start := time.Now()
// ... 处理请求 ...
duration := time.Since(start).Seconds()
requestDuration.Observe(duration)
```

**PromQL 查询**:

```promql
# P50 延迟
histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))

# P95 延迟
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# P99 延迟
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

### 4.2 Summary (摘要)

**原理**: 预先计算百分位数

```go
var requestDuration = prometheus.NewSummary(
    prometheus.SummaryOpts{
        Name: "http_request_duration_seconds",
        Help: "HTTP request latencies",
        Objectives: map[float64]float64{
            0.5: 0.05,   // P50 误差 5%
            0.9: 0.01,   // P90 误差 1%
            0.99: 0.001, // P99 误差 0.1%
        },
    },
)
```

**Histogram vs Summary**:

| 维度 | Histogram | Summary |
|------|-----------|---------|
| **聚合** | 可跨实例聚合 | 不可聚合 (每个实例独立) |
| **精度** | 近似 (桶误差) | 精确 (流式算法) |
| **内存** | 固定 (桶数量) | 可变 (数据量) |
| **查询** | 灵活 (任意百分位) | 固定 (预定义) |
| **适用** | 分布式系统 | 单实例监控 |

**推荐**: 分布式系统优先使用 **Histogram**！

### 4.3 日志采样

**问题**: 全量记录延迟数据会爆炸

**解决方案**: 采样策略

```python
import random

def should_sample(latency_ms):
    """
    自适应采样:
    - 快速请求 (< 100ms): 采样 1%
    - 慢请求 (100-500ms): 采样 10%
    - 超慢请求 (> 500ms): 采样 100%
    """
    if latency_ms < 100:
        return random.random() < 0.01
    elif latency_ms < 500:
        return random.random() < 0.1
    else:
        return True  # 全部记录

# 使用
latency = measure_request()
if should_sample(latency):
    log_to_elasticsearch(latency)
```

---

## 五、优化延迟的策略

### 5.1 分析延迟分布

**步骤 1: 确定问题范围**

```
查看监控:
- P50 正常，P99 高 → 偶发问题 (GC、网络抖动)
- P50 和 P99 都高 → 系统性问题 (代码、架构)
- P99.9 极高 → 极端情况 (冷启动、慢查询)
```

**步骤 2: 定位慢请求**

```python
# 记录慢请求详情
if latency > p99_threshold:
    logger.warning(
        "Slow request detected",
        extra={
            "latency_ms": latency,
            "url": request.url,
            "user_id": request.user_id,
            "db_query_time": db_time,
            "redis_time": redis_time,
            "external_api_time": api_time,
            "trace_id": trace_id,
        }
    )
```

**步骤 3: 分析慢请求特征**

```sql
-- 在日志系统 (如 Elasticsearch) 查询
SELECT
    url,
    COUNT(*) as slow_count,
    AVG(latency_ms) as avg_latency,
    MAX(latency_ms) as max_latency
FROM request_logs
WHERE latency_ms > 500
  AND timestamp > NOW() - INTERVAL 1 HOUR
GROUP BY url
ORDER BY slow_count DESC
LIMIT 10;
```

### 5.2 优化 P50 延迟

**目标**: 提升大部分用户的体验

**方法**:

```
1. 代码优化:
   - 减少数据库查询 (N+1 问题)
   - 算法优化 (O(n²) → O(n log n))
   - 并行处理 (goroutine, async/await)

2. 缓存:
   - Redis 缓存热点数据
   - 本地缓存常用配置
   - CDN 缓存静态资源

3. 数据库优化:
   - 添加索引
   - 查询优化 (EXPLAIN)
   - 连接池配置
```

**案例**:

```go
// 优化前: N+1 查询
users := db.Query("SELECT * FROM users")
for _, user := range users {
    orders := db.Query("SELECT * FROM orders WHERE user_id = ?", user.ID)
    // P50 = 200ms
}

// 优化后: JOIN 查询
result := db.Query(`
    SELECT u.*, o.*
    FROM users u
    LEFT JOIN orders o ON u.id = o.user_id
`)
// P50 = 20ms ✓ (10倍提升)
```

### 5.3 优化 P99 延迟

**目标**: 减少异常慢请求

**常见原因与解决方案**:

```
原因 1: GC 停顿
症状: 延迟突然飙升，持续数百毫秒
解决:
- 调优 GC 参数 (Go: GOGC, Java: -Xmx/-Xms)
- 减少对象分配 (对象池)
- 升级到低延迟 GC (G1, ZGC)

原因 2: 缓存未命中
症状: 偶发慢查询，P99 >> P50
解决:
- 缓存预热
- 提高缓存命中率
- 异步更新缓存

原因 3: 慢查询
症状: 特定查询极慢
解决:
EXPLAIN ANALYZE SELECT ...;
- 添加索引
- 分页查询
- 读写分离

原因 4: 网络抖动
症状: 不定期延迟
解决:
- 设置超时 (context.WithTimeout)
- 重试机制
- 熔断器 (Circuit Breaker)

原因 5: 冷启动
症状: 重启后第一批请求慢
解决:
- 预热缓存
- 预加载配置
- 连接池预先建立
```

### 5.4 微服务链路优化

**问题**: 调用链长导致延迟累积

**解决方案 1: 并行调用**

```go
// 串行调用 (慢)
user := getUserService(userID)       // 50ms
orders := getOrderService(userID)    // 50ms
products := getProductService(ids)   // 50ms
// 总延迟: 150ms

// 并行调用 (快)
var wg sync.WaitGroup
var user User
var orders []Order
var products []Product

wg.Add(3)
go func() {
    defer wg.Done()
    user = getUserService(userID)
}()
go func() {
    defer wg.Done()
    orders = getOrderService(userID)
}()
go func() {
    defer wg.Done()
    products = getProductService(ids)
}()
wg.Wait()
// 总延迟: max(50, 50, 50) = 50ms ✓ (3倍提升)
```

**解决方案 2: 超时控制**

```go
ctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)
defer cancel()

result, err := slowService(ctx)
if err == context.DeadlineExceeded {
    // 快速失败，返回降级结果
    return fallbackResult
}
```

**解决方案 3: 熔断器**

```go
// 使用 github.com/sony/gobreaker

cb := gobreaker.NewCircuitBreaker(gobreaker.Settings{
    Name: "ExternalAPI",
    ReadyToTrip: func(counts gobreaker.Counts) bool {
        failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)
        return counts.Requests >= 3 && failureRatio >= 0.6
    },
})

result, err := cb.Execute(func() (interface{}, error) {
    return callExternalAPI()
})
```

---

## 六、监控告警实战

### 6.1 Grafana Dashboard 配置

**Panel 1: 延迟趋势图**

```promql
# P50/P95/P99 延迟对比
histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

**Panel 2: 延迟热力图 (Heatmap)**

```promql
# 延迟分布热力图
sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
```

**Panel 3: 慢请求计数**

```promql
# 超过 500ms 的请求数
sum(rate(http_request_duration_seconds_count{le="0.5"}[5m]))
```

### 6.2 告警规则

```yaml
# Prometheus Alert Rules
groups:
  - name: latency_alerts
    interval: 30s
    rules:
      - alert: HighP99Latency
        expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "P99 latency is high ({{ $value }}s)"
          description: "99% of requests are taking more than 500ms"

      - alert: CriticalP99Latency
        expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 1.0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "P99 latency is critical ({{ $value }}s)"
          description: "Immediate action required"

      - alert: LatencyRegression
        expr: |
          histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
          /
          histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m] offset 1d))
          > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Latency regressed by 2x compared to yesterday"
```

---

## 七、实战案例

### 案例 1: 电商系统延迟优化

**背景**:
- 商品详情页 API
- P50 = 50ms (良好)
- P99 = 3s (糟糕!)

**分析**:

```
问题定位:
1. 日志分析: 99% 慢请求包含 "推荐商品" 逻辑
2. Trace 追踪: 推荐服务平均 50ms，但 P99 = 2.5s
3. 根因: 推荐算法偶尔触发复杂计算

解决方案:
- 推荐服务设置 200ms 超时
- 超时后返回缓存的推荐结果
- 异步预计算推荐结果

结果:
- P99 从 3s 降至 250ms ✓
- 用户体验大幅提升
```

### 案例 2: 微服务链路优化

**背景**:
- 用户首页需要调用 15 个微服务
- 每个服务 P99 = 100ms
- 整体 P99 = 1.5s (不可接受)

**优化**:

```
Step 1: 并行化
- 识别无依赖关系的调用
- 5 组并行调用，每组最多 3 个串行
- P99 降至 300ms

Step 2: 超时控制
- 所有调用设置 150ms 超时
- 超时返回降级数据 (缓存/默认值)
- P99 降至 180ms

Step 3: 预加载
- 用户登录时预加载热数据到缓存
- 首页直接读缓存
- P99 降至 50ms ✓
```

---

## 八、学习检查清单

完成以下检查后，说明你真正理解了延迟百分位数:

- [ ] 能解释为什么平均值不够，需要百分位数
- [ ] 能手工计算给定数据集的 P50/P95/P99
- [ ] 理解尾延迟放大效应 (微服务场景)
- [ ] 能用 Prometheus Histogram 监控延迟
- [ ] 会写 PromQL 查询百分位数
- [ ] 能设计基于 P99 的告警规则
- [ ] 知道如何优化 P50 和 P99 延迟
- [ ] 理解 Histogram vs Summary 的区别

---

## 参考资料

1. **Google SRE Book**: Chapter 4 - Service Level Objectives
2. **Tail at Scale** (Jeff Dean, Google): https://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale/fulltext
3. **How NOT to Measure Latency** (Gil Tene, Azul Systems)
4. **HdrHistogram**: http://hdrhistogram.org/

**下一步**: 学习 Little's Law → `资料3-Little定律详解.md`
