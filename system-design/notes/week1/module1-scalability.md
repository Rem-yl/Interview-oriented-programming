# Week 1 - Module 1: 可扩展性基础

> 学习时间: _____ 小时 | 日期: 2025-10-16

---

## 📚 《数据密集型应用系统设计》第1章 核心要点总结

### 第1章主题: 可靠、可扩展、可维护的应用系统

这一章是整本书的基础,定义了三个核心质量属性,它们是设计数据密集型应用的指导原则。

---

## 1️⃣ 可靠性 (Reliability)

### 核心定义

**即使出现故障,系统仍能正常工作**

### 关键概念

#### 故障 vs 失效

- **故障 (Fault)**: 系统的一个组件偏离其规格说明
- **失效 (Failure)**: 系统作为整体停止向用户提供服务
- **容错 (Fault-Tolerant)**: 能够应对故障的系统

💡 **设计原则**: 设计容错机制,在故障发生时防止失效

#### 三类故障

**1. 硬件故障 (Hardware Faults)**

- 硬盘崩溃、内存出错、电源中断、网络中断
- **传统方案**: 硬件冗余 (RAID、双电源、热插拔CPU)
- **现代方案**: 软件容错 + 硬件冗余
  - 云环境中虚拟机可能随时消失
  - 设计能在单机故障时继续运行的系统

**2. 软件错误 (Software Errors)**

- 系统性错误,比硬件故障更难预测
- 示例:
  - Linux 内核的闰秒 bug 导致大量应用挂起
  - 失控进程消耗所有 CPU/内存/磁盘/网络
  - 级联故障 (一个组件的小故障触发另一个组件的故障)
- **应对方法**:
  - 仔细考虑系统中的假设和交互
  - 彻底的测试
  - 进程隔离
  - 允许进程崩溃并重启
  - 监控和分析生产环境中的系统行为

**3. 人为错误 (Human Errors)**

- 人类是不可靠的 (配置错误是导致服务中断的首要原因)
- **应对策略**:
  - 最小化犯错机会的设计 (良好的抽象、API、管理界面)
  - 解耦容易犯错的地方 (提供沙盒环境,允许安全探索)
  - 各个层次的全面测试 (单元测试、集成测试、手动测试)
  - 快速恢复机制 (快速回滚、逐步发布新代码、数据重算工具)
  - 详细和清晰的监控 (性能指标、错误率)
  - 良好的管理实践和培训

---

## 2️⃣ 可扩展性 (Scalability)

### 核心定义

**系统应对负载增长的能力**

### 关键概念

#### 描述负载 (Load Parameters)

负载参数取决于系统架构,可能是:

- 每秒请求数 (QPS)
- 数据库读写比率
- 聊天室同时活跃用户数
- 缓存命中率

**经典案例: Twitter 的两个关键操作**

1. **发推文** (Post tweet)

   - 用户可发布消息给关注者 (平均 4.6k 请求/秒,峰值 12k 请求/秒)
2. **主页时间线** (Home timeline)

   - 用户查看关注者发布的推文 (300k 请求/秒)

**两种实现方式的权衡**:

**方式1: 查询时合并 (Pull 模式)**

```sql
SELECT tweets.*, users.* FROM tweets
JOIN follows ON follows.followee_id = tweets.sender_id
WHERE follows.follower_id = current_user
ORDER BY tweets.timestamp DESC
```

- 优点: 发推文操作简单
- 缺点: 主页时间线查询很重 (需要 join 大量数据)

**方式2: 写时扇出 (Push 模式)**

- 为每个用户维护一个缓存的主页时间线
- 用户发推文时,将新推文插入到所有关注者的时间线缓存中
- 优点: 主页时间线读取很快
- 缺点: 发推文的成本高,尤其对大V (粉丝众多的用户)

**Twitter 的混合方案**:

- 大多数用户的推文使用扇出
- 粉丝非常多的用户 (名人) 例外处理
- 读取时间线时,单独获取名人推文并与扇出的时间线合并

💡 **关键洞察**: Twitter 的瓶颈不是发推文的写入量,而是**扇出**带来的写放大

#### 描述性能 (Performance)

**两种思考方式**:

1. **增加负载,保持资源不变** → 性能如何受影响?
2. **增加负载,保持性能不变** → 需要增加多少资源?

**关键指标**:

- **吞吐量 (Throughput)**: 批处理系统关注 (如 Hadoop 每秒处理记录数)
- **响应时间 (Response Time)**: 在线系统关注

⚠️ **延迟 vs 响应时间**:

- **响应时间**: 客户端看到的时间 (包括网络延迟、排队延迟)
- **延迟**: 请求等待处理的时间 (awaiting service)

**百分位数 (Percentiles) 比平均值更重要**:

- **中位数 (P50)**: 一半请求响应时间低于此值
- **P95、P99、P99.9 (尾延迟)**: 95%、99%、99.9% 的请求响应时间低于此值

**为什么关注尾延迟?**

- 最慢的请求往往来自数据最多的用户 (最有价值的客户)
- Amazon 发现: 100ms 的延迟导致销售额减少 1%

**SLA (服务级别协议)** 示例:

- 服务被认为正常运行,如果中位数响应时间 < 200ms
- 99th 百分位 < 1s

**排队延迟 (Queueing Delays)**:

- 服务器只能并行处理少量请求
- 只需少量慢请求就能阻塞后续所有请求 (**头部阻塞, Head-of-line blocking**)

💡 **测试建议**: 客户端需要持续发送请求,而不是等待前一个请求完成

#### 应对负载的方法

**垂直扩展 (Scaling Up) vs 水平扩展 (Scaling Out)**

| 垂直扩展 (Scale Up) | 水平扩展 (Scale Out) |
| ------------------- | -------------------- |
| 升级到更强大的机器  | 将负载分布到多台机器 |
| 实现简单            | 架构复杂             |
| 成本非线性增长      | 成本线性增长         |
| 有物理上限          | 理论上无限           |

**现实架构**: 通常是混合方案

- 使用若干足够强大的机器 (降低复杂度)
- 分布到多个机器 (应对大规模)

**弹性系统 (Elastic System)**:

- 自动检测负载增加并添加计算资源
- vs 手动扩展

💡 **关键原则**: 没有通用的可扩展架构,每个系统的负载特征不同

- 读密集型 vs 写密集型
- 数据量、数据复杂度
- 响应时间要求
- 访问模式

---

## 3️⃣ 可维护性 (Maintainability)

### 核心定义

**让系统在生命周期内的维护工作变得简单**

软件成本的大部分不在初始开发,而在持续维护中:

- 修复 bug
- 保持系统运行
- 调查失效
- 适应新平台
- 针对新场景修改
- 偿还技术债
- 添加新功能

### 三个设计原则

#### 1. 可操作性 (Operability)

**让运维团队轻松保持系统平稳运行**

**运维团队的职责**:

- 监控系统健康,快速恢复服务
- 追踪问题根因 (系统故障、性能下降)
- 保持软件和平台更新 (安全补丁)
- 了解系统间的相互影响 (避免修改造成破坏)
- 预测未来问题 (容量规划)
- 建立良好的部署、配置管理实践
- 执行复杂的维护任务 (如平台迁移)
- 配置变更时维持系统安全
- 定义工作流程,使操作可预测
- 保持组织的知识传承

**良好的可操作性**:

- 提供良好的监控 (运行时行为和内部状态的可见性)
- 支持自动化和与标准工具集成
- 避免依赖单台机器 (允许机器下线维护)
- 良好的文档和易于理解的操作模型
- 良好的默认行为,允许管理员覆盖
- 自我修复,必要时给管理员手动控制
- 可预测的行为,减少意外

#### 2. 简单性 (Simplicity)

**让新工程师轻松理解系统**

**复杂性的症状**:

- 状态空间爆炸
- 模块间紧密耦合
- 纠结的依赖关系
- 不一致的命名和术语
- 针对性能的 hack
- 临时解决方案

**应对方法: 抽象 (Abstraction)**

- 隐藏大量实现细节
- 提供简洁、易于理解的外观
- 可被用于各种不同应用
- 高质量抽象可提高代码复用性

示例: 高级编程语言抽象了机器码、CPU 寄存器、系统调用
示例: SQL 抽象了复杂的磁盘/内存数据结构、来自其他客户端的并发请求、崩溃后的不一致

💡 **注意**: 简单 ≠ 简陋,好的抽象可以减少意外复杂度

#### 3. 可演化性 (Evolvability)

**让系统易于改变,适应需求变化**

别名: 可扩展性 (Extensibility)、可修改性 (Modifiability)、可塑性 (Plasticity)

**敏捷开发模式** 在软件层面提供了应对变化的框架:

- TDD (测试驱动开发)
- 重构

**系统设计层面**:

- 简单性和抽象是构建可演化系统的关键
- 易于理解的系统,才容易修改

---

## 🔑 核心洞察

### 1. 设计权衡无处不在

Twitter 的案例展示了经典的读写权衡:

- 优化读 (写时扇出) vs 优化写 (读时合并)
- 没有银弹,需要根据**负载特征**选择

### 2. 百分位数比平均值重要

- 平均值会掩盖真实的用户体验
- P99 延迟往往来自最有价值的用户 (数据最多)
- SLA 应该基于百分位数,而不是平均值

### 3. 复杂性是可维护性的大敌

- 意外复杂性 (Accidental Complexity) 源于实现
- 本质复杂性 (Essential Complexity) 源于问题本身
- 通过抽象减少意外复杂性

### 4. 没有通用的架构

- 每个系统的负载特征不同
- 需要根据实际情况设计
- 数据量、读写比例、复杂度都会影响架构选择

---

## 🤔 延伸思考

1. **Twitter 案例中,如果改用其他方案会怎样?**

   - 如果完全采用查询时合并,主页时间线的查询会成为瓶颈
   - 如果对所有用户都扇出,Lady Gaga 发推文时需要写入 6100 万个时间线
2. **为什么云时代更需要软件容错?**

   - 虚拟机可能随时被终止 (AWS Spot Instances)
   - 硬件故障率不变,但机器数量大幅增加
   - 需要设计"牛而非宠物"的系统 (Cattle, not Pets)
3. **可维护性的三个原则如何相互支持?**

   - 简单性 → 更容易操作和演化
   - 可操作性 → 减少维护负担,有时间重构
   - 可演化性 → 系统不断改进,变得更简单

---

## 📊 关键术语对照表

| 英文                  | 中文         | 定义                           |
| --------------------- | ------------ | ------------------------------ |
| Fault                 | 故障         | 单个组件偏离其规格说明         |
| Failure               | 失效         | 系统整体停止服务               |
| Scalability           | 可扩展性     | 应对负载增长的能力             |
| Load Parameters       | 负载参数     | 描述系统负载的指标             |
| Throughput            | 吞吐量       | 单位时间处理的记录数           |
| Response Time         | 响应时间     | 客户端发送请求到收到响应的时间 |
| Latency               | 延迟         | 请求等待处理的时间             |
| Percentile            | 百分位数     | X% 的值低于该阈值              |
| Tail Latency          | 尾延迟       | 高百分位的延迟 (如 P99)        |
| SLA                   | 服务级别协议 | 定义服务承诺的性能指标         |
| Fan-out               | 扇出         | 一次写操作触发多次写操作       |
| Head-of-line Blocking | 头部阻塞     | 慢请求阻塞后续请求             |
| Operability           | 可操作性     | 系统易于运维                   |
| Simplicity            | 简单性       | 系统易于理解                   |
| Evolvability          | 可演化性     | 系统易于改变                   |

---

## 📝 下一步行动

- [X] 阅读推荐资料: "Scalability for Dummies" 系列文章
- [X] 观看视频: "Horizontal vs Vertical Scaling"
- [X] 思考: 列出垂直扩展与水平扩展的对比表
- [X] 准备进入 CAP 定理的学习

---

## 🔗 相关资源

- 《Designing Data-Intensive Applications》第1章
- [Twitter&#39;s Fan-out Architecture](https://www.infoq.com/presentations/Twitter-Timeline-Scalability/)
- [Amazon: 100ms = 1% Sales Loss](https://www.fastcompany.com/1825005/how-one-second-could-cost-amazon-16-billion-sales)

---

## 🔺 CAP 定理深入理解

### 核心定义

**CAP 定理** (也称为 Brewer's Theorem) 指出:在一个分布式系统中,以下三个属性**不可能同时满足**,最多只能同时满足其中两个:

- **C (Consistency)** - 一致性
- **A (Availability)** - 可用性
- **P (Partition Tolerance)** - 分区容错性

### 三个属性的详细解释

#### 1. 一致性 (Consistency)

**定义**: 所有节点在同一时间看到相同的数据

**更准确的定义**: 线性一致性 (Linearizability)
- 一旦写入成功,所有后续读取都能看到该写入
- 系统表现得像只有一个数据副本

**用户视角**:
- 写入数据后立即读取,必定能读到刚写入的值
- 不会出现"旧数据"

**示例**:
```
时间线: 用户A写入 x=5 → 用户B读取 x → 必须返回 5 (不能是旧值)
```

#### 2. 可用性 (Availability)

**定义**: 每个请求都能收到响应(成功或失败),但不保证数据是最新的

**更准确的定义**:
- 系统中的每个**非故障节点**都必须在合理时间内返回响应
- 不允许无限期等待

**用户视角**:
- 请求不会挂起,总能得到响应
- 响应可能是成功或失败,但不会超时

**重要**: 可用性不等于"系统不宕机",而是"能响应的节点必须响应"

#### 3. 分区容错性 (Partition Tolerance)

**定义**: 系统在网络分区(部分节点间通信中断)的情况下仍能继续运行

**网络分区示例**:
```
节点A <--X--> 节点B  (网络断开)
节点A <-----> 节点C  (网络正常)
节点B <-----> 节点C  (网络正常)
```

**现实情况**:
- 网络分区在分布式系统中**不可避免**
- 网络延迟、丢包、机房断电、光纤被挖断
- 因此,分布式系统**必须容忍分区** (P 不是可选项)

---

### 为什么只能三选二?

#### 网络正常时: CAP 都能满足

当网络正常,所有节点都能通信时,可以同时实现 C、A、P

#### 网络分区时: 必须在 C 和 A 之间选择

**场景**: 节点 A 和节点 B 之间网络断开

**选择 1: 优先一致性 (CP)**
```
用户向节点A写入 x=5
节点A无法同步到节点B (网络分区)

CP 系统的做法:
- 节点A 拒绝写入 (或等待,直到网络恢复)
- 保证一致性,但牺牲可用性
```

**选择 2: 优先可用性 (AP)**
```
用户向节点A写入 x=5
节点A无法同步到节点B (网络分区)

AP 系统的做法:
- 节点A 接受写入,返回成功
- 节点B 暂时看到旧数据 (不一致)
- 等网络恢复后再同步
- 保证可用性,但牺牲一致性
```

---

### CAP 权衡的真实案例

#### CP 系统: 银行转账系统

**为什么选择 CP**:
- 一致性至关重要(账户余额不能错)
- 宁可拒绝服务,也不能出现数据不一致

**具体表现**:
- 网络分区时,部分节点拒绝写入
- 用户可能收到"系统暂时不可用"错误
- 等待网络恢复后才能继续服务

**代表系统**:
- 传统关系型数据库 (PostgreSQL、MySQL with sync replication)
- Zookeeper (强一致性的协调服务)
- etcd (Raft 协议保证一致性)
- HBase

#### AP 系统: 社交网络的点赞功能

**为什么选择 AP**:
- 可用性更重要(用户不能容忍长时间等待)
- 短暂的不一致可以接受

**具体表现**:
- 你点赞后,朋友可能暂时看不到
- 几秒钟后最终一致
- 系统始终可用,不会拒绝服务

**代表系统**:
- Cassandra (最终一致性)
- DynamoDB (可调一致性,默认 AP)
- Riak
- Couchbase

#### CA 系统: 不存在于分布式环境

**为什么不存在**:
- 分布式系统**必然**面临网络分区
- 不能容忍分区 = 单机系统

**CA 的实际意义**:
- 单机数据库(PostgreSQL、MySQL)在单机上是 CA
- 但一旦做了分布式,就必须选择 CP 或 AP

---

### CAP 三角形与常见系统

```
          一致性 (C)
            /\
           /  \
          /    \
         / CP区 \
        /  域    \
       /          \
      /   Zookeeper\
     /     etcd     \
    /    HBase       \
   /                  \
  /_____________________\
P                        A
分区容错                可用性

   AP 区域:
   - Cassandra
   - DynamoDB
   - Riak
```

**常见系统的 CAP 选择**:

| 系统 | CAP类型 | 说明 |
|------|---------|------|
| Zookeeper | CP | 强一致性,网络分区时少数派不可用 |
| etcd | CP | 基于 Raft,保证线性一致性 |
| Redis Sentinel | CP | Master 选举保证一致性 |
| MongoDB (默认) | CP | 主节点写入,强一致性 |
| Cassandra | AP | 最终一致性,高可用 |
| DynamoDB | AP (可调) | 默认最终一致,可选强一致 |
| Riak | AP | 基于 Dynamo 论文,最终一致 |
| PostgreSQL (单机) | CA | 单机不涉及分区问题 |
| MySQL (单机) | CA | 单机不涉及分区问题 |

---

### CAP 定理的现代理解

#### Eric Brewer 的修正 (CAP Twelve Years Later, 2012)

1. **P 是必选项,不是可选项**
   - 分布式系统必须容忍分区
   - 真正的选择是 C 和 A 之间的权衡

2. **不是非黑即白,而是程度问题**
   - 不是完全放弃 C 或 A
   - 而是在不同场景下做不同的权衡

3. **分区是暂时的,大部分时间没有分区**
   - 分区期间: 选择 C 或 A
   - 分区恢复后: 修复不一致状态

4. **可以针对不同操作做不同选择**
   - 关键操作(转账): 选择一致性
   - 非关键操作(浏览): 选择可用性

#### 一致性的谱系 (不是非黑即白)

从强到弱排序:

1. **线性一致性 (Linearizability)** - 最强
   - CAP 定理中的 C
   - 所有操作看起来是瞬间完成的

2. **顺序一致性 (Sequential Consistency)**
   - 所有节点看到相同的操作顺序
   - 但操作可能有延迟

3. **因果一致性 (Causal Consistency)**
   - 有因果关系的操作必须按顺序
   - 无因果关系的操作可以乱序

4. **最终一致性 (Eventual Consistency)** - 最弱
   - 没有写入时,最终所有节点会达到一致
   - CAP 定理中 AP 系统通常采用这种

---

### 真实场景的 CAP 选择

#### 场景 1: 电商库存系统

**问题**: 商品只剩 1 件,两个用户同时下单

**CP 方案** (推荐):
- 使用分布式锁
- 第一个用户成功,第二个用户失败
- 保证不会超卖

**AP 方案** (不推荐):
- 两个用户都下单成功
- 后台检测到超卖,取消一个订单
- 用户体验差

**结论**: 库存扣减必须选择 CP

#### 场景 2: 社交网络的粉丝数

**问题**: 用户关注/取关时,粉丝数可能暂时不准确

**AP 方案** (推荐):
- 接受短暂的不一致
- 粉丝数 99.9K 和 100K 差别不大
- 用户更在意操作不卡顿

**CP 方案** (不推荐):
- 为了粉丝数绝对准确,牺牲可用性
- 用户体验差

**结论**: 粉丝数可以选择 AP

#### 场景 3: 支付系统

**问题**: 扣款和到账必须原子性

**CP 方案** (必须):
- 使用分布式事务 (2PC、3PC、Saga)
- 宁可失败,不能出现钱丢失或重复扣款

**结论**: 金融系统必须 CP

---

### 突破 CAP: 实践中的优化

#### 1. 可调一致性 (Tunable Consistency)

**Cassandra 的 Quorum 机制**:

```
写入时: W = 写入成功的副本数
读取时: R = 读取的副本数
总副本: N

强一致性: W + R > N
最终一致: W + R <= N
```

**示例**: N=3 (三个副本)

| W | R | 一致性 | 说明 |
|---|---|--------|------|
| 3 | 1 | 强一致 | 写入所有副本,读取一个 |
| 2 | 2 | 强一致 | W+R=4 > N=3,必有交集 |
| 1 | 1 | 最终一致 | 最低延迟,但可能读到旧数据 |

**灵活性**: 根据操作重要性,动态调整 W 和 R

#### 2. 分区期间降级,分区恢复后修复

**策略**:
- 分区期间: 放宽一致性,保证可用性
- 分区恢复: 运行冲突解决算法(向量时钟、CRDT)

**示例**: Amazon 购物车
- 分区期间: 允许多个版本的购物车
- 恢复后: 合并购物车(取并集,宁可多不可少)

#### 3. 混合架构

**不同数据选择不同策略**:
- 用户余额(关键数据): CP
- 用户浏览历史(非关键): AP

---

### 关键洞察

1. **P 不是可选的**
   - 网络分区是现实,不是假设
   - 所有分布式系统都必须容忍分区

2. **CAP 是权衡,不是绝对**
   - 不是"完全放弃一致性"或"完全放弃可用性"
   - 而是在不同场景下做不同程度的权衡

3. **业务需求决定选择**
   - 金融系统: 一致性 > 可用性
   - 社交网络: 可用性 > 一致性

4. **可以在不同层次做不同选择**
   - 同一系统的不同功能,可以有不同的 CAP 选择

---

### 延伸思考题

1. **为什么 Zookeeper 选择 CP 而不是 AP?**
   - Zookeeper 用于分布式协调(锁、配置)
   - 配置必须一致,否则会导致系统错误
   - 宁可暂时不可用,也不能返回错误配置

2. **为什么 DNS 选择 AP 而不是 CP?**
   - DNS 查询量巨大,必须高可用
   - 短暂的不一致可以接受(TTL 机制)
   - 如果 DNS 不可用,整个互联网瘫痪

3. **如果你设计微信朋友圈,如何选择?**
   - 发布朋友圈: AP (宁可延迟可见,不能拒绝发布)
   - 点赞/评论: AP (最终一致即可)
   - 红包金额: CP (不能出现超发或丢失)

---

### CAP 定理练习

**练习 1: 判断以下系统的 CAP 类型**

1. Google Docs 多人协作编辑: _____ (AP,倾向可用性)
2. 12306 火车票抢票: _____ (CP,不能超卖)
3. 微博热搜榜: _____ (AP,实时性不绝对)
4. 区块链(比特币): _____ (CP,强一致性)

**练习 2: CAP 三角形标注**

在 CAP 三角形上标注以下系统的位置:
- Redis (AP 偏向)
- PostgreSQL with sync replication (CP)
- Cassandra (纯 AP)
- MongoDB (CP 偏向)

---

**学习收获**: _____________________

**待深入问题**: _____________________
